//To fill out this template, replace everything in < > brackets (including the brackets

{
	"task": "<your task name>",   // You can choose anything you want here. Typically, it is the name of the graph formalism (e.g. "AMR").
	"evaluation_command" : {
		"type" : "bash_evaluation_command",
			"command" : "< your evaluation command here>",
			// This command is a bash command whose job it is to run the evaluation given an output amconll file. So you first call the EvaluateAMConll script of am-tools, and if your
			// EvaluationToolset does not run the actual comparison to the gold files, run your applicable evaluation script (such as smatch for AMR) afterwards (separate multiple
			// commands in the bash command with "&&" as usual). Make sure that the output metrics of this command are printed to the command line; the result_regexes below will fetch them from there.
			// In this command, you can use the literal "{system_output}" for the path to the amconll file that serves as input here (this file was generated by the parser) and "{gold_file}"f as the path to the gold data.
			// and "{tmp}" for a temporary directory where you can store intermediate files. See the other files in this folder for examples.
			"result_regexes" : {"<metric name here>" : [<line number here>, "<regex here>"],
							"<metric name here>" : [<line number here>, "<regex here>"]}
			// The result regexes extract metrics from the evaluation command output. For each metric that you want to extract, you specify a metric name (of your choice), a 0-based line number
			// (to which line of the output the regular expression should be applied) and a regular expression. The regular expression must contain a named capturing group with name "value",
			// i.e. a string of the form "(?P<value>group)" where you replace "group" with the actual regular subexpression.
			// For example: a result regex entry of the form ["F" : [2, "F-score: (?P<value>.+)"] will match the third line of the output (recall line numbers are 0-based). If that third line is
			// F-score: 78.34
			// Then the named capturing group will match "78.34", which is then parsed as a float and stored as a metric. Metric names are prefixed with "<your task name>_" downstream,
			// so assuming the task name is "AMR", then this metric will show up as "AMR_F". In comet.ml, for the dev set there is an additional "validate_" prefix, yielding "validate_AMR_F" in this example (similar for the test set).
 	},
    "validation_metric" : "<your choice of validation metric>",   // Determines which validation metric is used for choosing the best epoch (and early stopping if applicable).
	// Start the string with "+" if higher value of the metric is better, and "-" if lower is better. You can use a name from the result_regexes above, if you prefix it with "<your task name>_".
	// Examples: "-loss" or "+AMR_F" where in the latter case <your task name> is "AMR" and "F" is the metric name of one of the entries in result_regexes.
}