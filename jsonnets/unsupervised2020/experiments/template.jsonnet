# To make this work for your graph formalism, replace everything in < > brackets (including the brackets). Feel free to adjust the numbers in the top header to suit your dataset.

#============TRAINING PARAMETERS==============
local batch_size = 32; # adjust this to fit the memory capacity of your machine (mostly GPU memory is relevant)
local num_epochs = 100; # we recommend 40-100 epochs on real datasets and 200 epochs for toy datasets, but your optimal parameters may differ
local patience = 1000; # this is per default set to 1000 to turn off early stopping.
local evaluate_on_test = false; # Whether to evaluate on the test set. Set to true when you are ready to evaluate on test rather than just on dev.


#============FILEPATHS==============
local train_zip_path = "<train.zip file>";  # as generated by de.saar.coli.amtools.decomposition.SourceAutomataCLI in am-tools
local dev_zip_path = "<dev.zip file>";  # as generated by de.saar.coli.amtools.decomposition.SourceAutomataCLI in am-tools
local validation_amconll_path = "<dev.amconll file>";  # as generated by de.saar.coli.amtools.decomposition.CreateEvaluationInput
local validation_gold_path = "<your dev gold files>";  # dev gold files, in the format that the evaluation command in the formalism_config (see below) reads
local test_triples_amconll_gold_suffix = [["<test.amconll file>", "<gold_text.txt file>", ""]];  # each triple consists of (a) path to amconll input for test set; (b) path to gold output for test set; (c) a suffix to be attached to the names of the evaluation metrics. (a) and (b) are like for the dev set in the lines above. Use multiple triples to evaluate on multiple test sets. In the regular case of just using one test set, use just one triple and the suffix can be the empty string. You can also use no test set (the usual case during model development); in that case leave this list empty and set evaluate_on_text to false above.


#=============IMPORTING FORMALISM AND MODEL CONFIGS==================
local formalism_config = import "<path to formalism config file>"; # The formalism-specific config file. Path can be relative to this file. 
# A template for that file is in ../formalisms/template.libsonnet. It is highly recommended to look at some examples in that formalisms folder.

local raw_model_config = import '../models/default2021.libsonnet';  # You can typically use one of the predefined models here. Use ../models/toy.libsonnet for
# experimenting on a toy dataset (say 1-10 sentences) and ../models/default2021.libsonnet for a full dataset.
# To tune the model's hyperparameters, look at this raw_model_config file.





#==================PUTTING IT ALL TOGETHER (DO NOT MODIFY) ===================
# This puts everything above together. You should not need to modify anything below this line

# Putting the parameters of this file and the formalism config into the model config file (this is necessary due to how the jsonnet structures are organized/nested)
local model_config = raw_model_config(batch_size, num_epochs, patience, formalism_config['task'], formalism_config['evaluation_command'], formalism_config['validation_metric'], validation_amconll_path, validation_gold_path, test_triples_amconll_gold_suffix);

# Now we can write down all of the actual config entries
{
	"dataset_reader": model_config['dataset_reader'],
    "iterator": model_config['iterator'],
    "vocabulary" : model_config['vocabulary'],
    "model":  model_config['model'],
	
    "train_data_path": [ [formalism_config['task'], train_zip_path]],
    "validation_data_path": [ [formalism_config['task'], dev_zip_path]],


    #=========================EVALUATE ON TEST=================================
    "evaluate_on_test" : evaluate_on_test,
    "test_evaluators" : model_config['test_evaluators'],
    #==========================================================================

    "trainer" : model_config['trainer'],
}

